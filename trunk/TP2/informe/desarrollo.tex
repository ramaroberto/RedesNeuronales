\documentclass[informe.tex]{subfiles}
\begin{document}
  
  \section{Desarrollo}
    \subsection{Reducción de dimensionalidad}
      \subsubsection{Análisis de componentes principales}
      \subsubsection{Reglas de Oja y Sanger}
	La regla de Oja, encuentra un vector de pesos unitario que maximiza el output de la cuadratica media. Para datos con media cero este vector es solamente la componente principal. De todas formas, sería deseable tener una red de M-outputs que extraiga las primeras M componentes principales. Sanger y Oja diseñaron redes neuronales de una sola capa y feed-forward para análisis de componentes principales que hacen esto.
	
	~
	
	La regla de aprendizaje de Sanger es:

	$$\Delta w_{ij} = \eta V_i(\xi_j \sum_{k=1}^{i} V_k w_{kj} )$$

	Mientras que la de Oja es:

	$$\Delta w_{ij} = \eta V_i(\xi_j \sum_{k=1}^{N} V_k w_{kj} )$$

	La única diferencia entre ambas es el límite superior de la sumatoria. En ambos casos los $w_i$ vectores convergen a vectores unitarios ortogonales, tales que $w^{T}_i w_j = \delta_{ij}$. En el caso de la regla de Sanger, los vectores de pesos se vuelven exactamente con las primeras M componentes principales, y es por esto que es mas util en aplicaciones ya que extrae los componentes principales en orden. Como nota de color, si algun algoritmo de este tipo es implementado en cerebros reales, probablemente sea mas parecido a la regla de Oja: no hay ninguna ventaja obvia para un animal en tener la informacion ordenada en base a los componentes principales. 

	De todas formas, en ambos casos la salida proyecta un vector de entrada $\xi$ en el espacio de las $M$ componentes.
    \subsection{Mapeo de caracteristicas auto-organizado}
  
\end{document}