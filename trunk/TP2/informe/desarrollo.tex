\documentclass[informe.tex]{subfiles}
\begin{document}
  
  \section{Desarrollo}
    \subsection{Reducción de dimensionalidad}
      \subsubsection{Análisis de componentes principales}
	El an\'alisis de componentes principales es un proceso estadístico que usa una transformaci\'on ortogonal que convierte observaciones en un conjunto de variables linearmente no correlacionadas llamadas componentes principales. La transformación se define de modo que la primera componente principal tiene la mayor varianza entre todas las variables y cada una de las siguientes componentes tiene la mayor varianza posible entre todas las ortogonales a las componentes anteriores. Luego, el objetivo es calcular una cierta cantidad de componentes principales que contengan las mayor información respecto de los datos para poder expresarlos en función de dichos vectores pertenecientes a una base ortonormal.
	
      \subsection{Regla de Oja}
	La regla de Oja es una modificacion a la regla de aprendizaje Hebbiano que previene la divergencia. Mediante la modificacion, el vector de pesos se aproxima a una longitud constante $|w|=1$, sin la necesidad de realizar ninguna normalizacion. Por otra parte, el vector $w$ ciertamente se acerca al autovector $C$ con el autovalor $\delta_{max}$ mas alto, es decir el autovector principal. Esta regla tambien es llamada ``Oja1'', ya que solamente nos da el primer autovector o componente principal.
	
	~
	
	Como en este trabajo nos interesa las primeras tres componentes principales, no analizaremos la eficacia de esta regla, si no que iremos con su version $M$.
      
      \subsubsection{Reglas de Oja y Sanger}
	Como expresamos anteriormente, es deseable tener una red de M-salidas que extraiga las primeras M componentes principales. Sanger y Oja diseñaron redes neuronales de una sola capa y feed-forward para análisis de componentes principales que se encargan de llevar a cargo esta tarea.
	
	~
	
	La regla de aprendizaje de Sanger es:

	$$\Delta w_{ij} = \eta V_i(\xi_j \sum_{k=1}^{i} V_k w_{kj} )$$

	Mientras que la de Oja es:

	$$\Delta w_{ij} = \eta V_i(\xi_j \sum_{k=1}^{N} V_k w_{kj} )$$

	La única diferencia entre ambas es el límite superior de la sumatoria. En ambos casos los $w_i$ vectores convergen a vectores unitarios ortogonales, tales que $w^{T}_i w_j = \delta_{ij}$. En el caso de la regla de Sanger, los vectores de pesos se aproximan exactamente a las primeras $M$ componentes principales, y es por esto que es mas \'util en aplicaciones ya que extrae los mismos en orden. Como nota de color, si algun algoritmo es utilizado en cerebros de seres viviantes, probablemente sea mas parecido a la regla de Oja: no hay ninguna ventaja obvia para un ser en tener la informacion ordenada en base a los componentes principales. 

	De todas formas, en ambos casos la salida proyecta un vector de entrada $\xi$ en el espacio de las $M$ componentes.
	
      \subsubsection{Criterios de convergencia}
	Para determinar que el algoritmo convergió a vectores ortogonales correspondientes a las primeras componentes principales, utilizamos dos criterios diferentes. Por un lado, calcular el producto entre los vectores obtenidos en cada iteración y ver que el resultado sea menor a un epsilon pequeño. Si todos los productos de los vectores entre sí son pequeños entonces se puede decir que los mismos son suficientemente ortogonales y entonces dejar de iterar. El otro criterio considerado observa los valores de los $\delta W$. Si dicha matriz tiene una norma Frobenius menor a un epsilon pequeño entonces se asume que el aprendizaje que se puede obtener de allí es mínimo y por ende no tiene sentido continuar iterando.
	
    \subsection{Mapeo de caracteristicas auto-organizado}
	Los mapas de características auto-organizados sirven para generar una representación de un espacio de muestras de gran dimensionalidad en otra de menor preservando propiedades topológicas del espacio de entrada. De alguna manera podr\'ia decirse que los SOMs (Self-Organizing Maps) son la versión en dos dimensiones de PCA (Principal Component Analysis). Una vez entrenado el mapa, dada una instancia nueva se busca cuál neurona se activa más ante el estímulo que esa entrada genera y entonces se le asigna la clase correspondiente a la clase que más activó esa neurona en el período de entrenamiento.
	
	\subsubsection{Autoajuste de los parámetros de aprendizaje}
	  Para la experimentación decidimos considerar también la posibilidad de utilizar learning rate y $\sigma$ autoajustables según la época. La opción de autoajuste consiste en: $\alpha = t^{-0.5}$ y $\sigma = \frac{M_2}{2}t^{-\frac{1}{3}}$ donde $\alpha$ es el learning rate, $t$ es la época actual y $\sigma$ es el coeficiente que varía las amplitudes del efecto de propagación.
\end{document}