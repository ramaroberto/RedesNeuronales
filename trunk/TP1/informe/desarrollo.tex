\documentclass[informe.tex]{subfiles}
\begin{document}
  
  \section{Desarrollo}
  
  \subsection{Implementacion de la red neuronal}
    La implementacion de la red neuronal se realizo en MatLab. Decidimos que era el entorno mas apropiado para el mismo, ya que por tratarse de un trabajo de experimentacion no estabamos buscando performance y nos brinda diversas comodidades en operaciones con matrices. 
    
    Se utilizo la implementacion de la red neuronal como se presento en clase, y se agrupo cada parte de la misma en una clase definida en MatLab, bajo el nombre de \textbf{MyMultiPerceptron}. Puntualmente aplicamos aprendizaje incremental para el entrenamiento e implementamos la modificacion del \textbf{momentum} para que la red pueda converger mas rapido. Tanto el momentum ($\alpha$) como el learning rate ($\gamma$) no se modifican durante el entrenamiento. 
    
    A modo de dejar bien asentada la implementacion, la presentamos a continuacion en formato de pseudocodigo. Sin embargo no daremos mayores explicaciones ya que no presenta ninguna innovacion. La funcion de activacion fue reemplazada correspondientemente segun el modo en el que la red neuronal estuviese corriendo, daremos mas detalles en la seccion que especifica sus opciones.
  
    \vspace{15pt}
    
      \begin{minipage}{0.5\textwidth}
	  \begin{algorithmic}
	    \Function{feedForward}{w, $x$} 
	      \State $y =$ propagateFeed(w, $x$)
	      \State  \Return $y_{|y|}$
	    \EndFunction
	  \end{algorithmic}
	  
	  \vspace{5pt}
	  
	  \begin{algorithmic}
	  \Function{propagateFeed}{w, $x$}
	    \State $y_1 = x$
	    \For{$i = [1..|$w$|]$}
	      \State $y_{i+1} = $activation($[y_i\ 1]\ *\ $w$_i$)
	    \EndFor
	    \State \Return $y$
	    \EndFunction
	  \end{algorithmic}
	  
	  \vspace{5pt}
	  
	  \begin{algorithmic}
	  \Function{train}{w, data, err$_{min}$, ep$_{max}$, $\gamma$, $\alpha$}
	    \State $\Delta lw = \emptyset$
	    \While{$e\ >$ err$_{min}\ \wedge$ $t\ <$ ep$_{max}$}
	      \State $e = 0$
	      \State $t = t+1$
	      \For{$x,z = [1..|$data$|]$}
		\State $y =$ propagateFeed($x$)
		\State $[fe\ \Delta w] = $ correction(w, $y$, $z$, $\gamma$)
		\State w $=$ adaptation(w, $\Delta w$, $\Delta lw$, $\alpha$)
		\State $\Delta lw = \Delta w$
		\State $e = e + fe$
	      \EndFor
	    \EndWhile
	  \EndFunction
	  \end{algorithmic}
      \end{minipage}
      \begin{minipage}{0.5\textwidth}
	  \begin{algorithmic}
	  \Function{correction}{w, $y$, $z$, $\gamma$}
	    \State $\Delta w = \emptyset$
	    \State $re = z - y_{|y|}$
	    \State $fe = ||re||_1 / |re|$
	    \For{$i = [|w|..1]$}
	      \State $re = re\ .*$ activation'($y_{i+1}$)
	      \State $\Delta w_i = \gamma\ *\ [y_i\ 1]'\ *\ re$
	      \State $re = re\ *$ w$_{[1..|w|-1]}$
	    \EndFor
	    \State \Return $[fe\ \Delta w]$
	  \EndFunction
	  \end{algorithmic}
	  
	  \vspace{5pt}

	  \begin{algorithmic}
	  \Function{adaptation}{w, $\Delta w$, $\Delta lw$, $\alpha$}
	    \For{$i = [1..|w|]$}
	      \State w$_i =$ w$_i + \Delta w + \alpha * \Delta lw$
	    \EndFor
	    \State \Return w
	  \EndFunction
	  \end{algorithmic}
	  
	  \vspace{100pt}
      \end{minipage}
    
    \vspace{15pt}

  \subsection{Preprocesamiento de los datos}
  
    A fin de lograr que todas las características observadas tengan la misma influencia en la red (y evitar que la influencia de alguna característica con amplitud y valor absoluto en sus valores sea mayor que la de una con análogos menores), analizamos los resultados al hacer un preprocesamiento de cada caraterística en las bases de datos.
    
    Como preprocesamiento elegimos realizar una normalización de cada característica numérica, incluyendo la salida en el segundo conjunto de datos.
  
  
  \subsection{Generación de instancias de prueba}

    Con el objetivo de evaluar diferentes arquitecturas de redes decidimos particionar el conjunto de datos disponibles (realizando la misma operación para cada dataset) utilizando k-fold cross-validation para disminuir los efectos de overfitting y los ocasionados por tomar conjuntos poco representativos y analizar el funcionamiento de una determinada selección de parámetros sobre distintas particiones de los datos. Sin embargo, considerando que debemos realizar una serie de pruebas en un tiempo acotado, decidimos acotarlas de la siguiente manera.
    
    Inicialmente, particionamos la totalidad de los datos en cuatro partes, cada una considerando el 25\% de los mismos. Dada esa partición generamos 4 folds diferentes. Luego, para cada conjunto de 75\% de los datos realizamos una nueva partici\'on en tres partes iguales para realizar cross-validation en la proporci\'on 66\% para entrenamiento y 33\% para test. 
    
    De esta manera, analizamos diferentes arquitecturas sobre el 50\% de los datos (dos tercios de tres cuartos) testeando en unos de los 25\% restantes con el fin de elegir aquellas con buen desempeño. Una vez elegidas, realizamos una nueva etapa de entrenamiento ahora sobre el 75\% de los datos (usados previamente para entrenar y testear) que ser\'an validados en el 25\% restante ya como m\'etodo de validaci\'on final entre aquellas redes obtenidas con parámetros que generaron buenos resultados en la etapa previa.
  
\end{document}